<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SIGNAL Lab</title>
    <description>Situated Grounding and Natural Language &lt;br/&gt; NLP @ CSU</description>
    <link>https://www.signallab.ai/https://www.signallab.ai//</link>
    <atom:link href="https://www.signallab.ai/https://www.signallab.ai//feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 18 May 2025 14:29:54 +0000</pubDate>
    <lastBuildDate>Sun, 18 May 2025 14:29:54 +0000</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>New ARPA-H and ARO Research Awards</title>
        <description>&lt;p&gt;Despite all the drama with funding at the federal level, I’m fortunate to have received not one, but two new research funding awards recently.&lt;/p&gt;

&lt;p&gt;The first comes from the Advanced Research Projects Agency for Health (ARPA-H), the Department of Health and Human Services’s in-house innovator (basically DARPA for health). The Platform Accelerating Rural Access to to Distributed and Integrated Medical Care (PARADIGM) program seeks to improve healthcare outcomes for rural Americans by providing hospital-level care on-site (such as at people’s homes or town centers) via a mobile care delivery platform (CDP – think a mobile clinic in a smart van).&lt;/p&gt;

&lt;p&gt;Our team is called &lt;em&gt;VIGIL: Vectors of Intelligent Guidance in Long-Reach Rural Healthcare&lt;/em&gt; (like ARPA-H we play a little fast and loose with acronyms). We are part of a large team led by Prof. Jason Corso at the University of Michigan, and our role is to develop AI-powered task guidance technology that can upskill a local health practitioner to provide certain hospital level procedures via dynamic multimodal guidance.&lt;/p&gt;

&lt;p&gt;The other award I received comes from the Army Research Office Knowledge Systems Program and is called &lt;em&gt;Modeling Causality in AI Through Embodiment and Analogy&lt;/em&gt;, and we seek to imbue LLMs with correct and verifiable physical causal reasoning properties by sourcing high-fidelity physical knowledge from embodied simulation environments (Unity) in problem-solving scenarios. This follows on previous work funded by ARO, an NSF EAGER grant from 2020-2022, and work done in my Ph.D. and postdoc.&lt;/p&gt;

&lt;p&gt;Delighted to have received this funding, especially at this time!&lt;/p&gt;

&lt;p&gt;(X-posted on &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/2025/02/13/new-arpa-h-aro-research-awards.html&quot;&gt;nikhilkrishnaswamy.com&lt;/a&gt;)&lt;/p&gt;
</description>
        <pubDate>Thu, 13 Feb 2025 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//new-arpa-h-aro-research-awards</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//new-arpa-h-aro-research-awards</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>grants</category>
        
        
      </item>
    
      <item>
        <title>Best Paper Nominee at HAI 2024 and AAAI Demo</title>
        <description>&lt;p&gt;Congratulations to Ph.D. student Sheikh Mannan and former SIGNAL Lab undergraduate researcher Paige Hansen! Our paper, &lt;em&gt;Combating Spatial Disorientation in a Dynamic Self-Stabilization Task Using AI Assistants&lt;/em&gt;, received a nomination for Best Paper at the International Conference on Human-Agent Interaction 2024. This paper was written in collaboration with the Department of Psychology and Neuroscience at Brandeis University. In it, we use a high-throughput, high-fidelity simulation of inverted pendulum balancing under spatially disorienting conditions and train a variety of AI models that can perform the task to various different abilities, and also provide cues to humans performing the task in order to enable them to take better actions. We conducted a high-volume study using digital twins and then a human subject study in which 20 subjects and 5 of the best performing assistant models engaged in human-in-the-loop training of the assistants. Our findings indicate that AI assistance can help people perform substantially better at the task, but that assitance who have a more human-like mode of operation are less effective yet more trusted and preferred by humans.&lt;/p&gt;

&lt;p&gt;Congratulations to Mannan and Paige!&lt;/p&gt;

&lt;p&gt;The link to the proceedings can be found &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3687272.3688329&quot;&gt;here&lt;/a&gt;. The platform we developed for this study will also be presented as a demo at AAAI 2025 next February in Philadephia!&lt;/p&gt;

&lt;p&gt;Citations:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Mannan, S., Hansen, P., Vimal, V.P., Davies, H.N., DiZio, P., and Krishnaswamy, N. (2024). Combating Spatial Disorientation in a Dynamic Self-Stabilization Task Using AI Assistants. In &lt;em&gt;International Conference on Human-Agent Interaction (HAI)&lt;/em&gt;. ACM.&lt;/li&gt;
  &lt;li&gt;Mannan S. and Krishnaswamy, N. (2025). Bidirectional Human-AI Learning in Real-Time Disoriented Balancing. To appear in &lt;em&gt;AAAI Conference on Artificial Intelligence (AAAI): Demos Program&lt;/em&gt;. AAAI.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 05 Dec 2024 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//best-paper-nominee-hai-2024</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//best-paper-nominee-hai-2024</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>awards</category>
        
        
      </item>
    
      <item>
        <title>Congratulations to Shadi Manafi!</title>
        <description>&lt;p&gt;Congratulations to Dr. &lt;strong&gt;Shadi Manafi&lt;/strong&gt; for successfully defending her Ph.D. dissertation!&lt;/p&gt;

&lt;p&gt;In Shadi’s dissertation, &lt;em&gt;Smart Transfers:  Challenges and Opportunities in Boosting Low-Resource Language Models with High-Resource Power&lt;/em&gt;, she examines the properties of transformer language models trained on well-resourced or high-resource languages (HRLs) when applied to low-resource languages (LRLs) and finds that language models can come to depend to an extent on word memorization of the better-resourced languages, a shortcoming that comes with some unexpected drawbacks and (surprisingly) benefits.&lt;/p&gt;

&lt;p&gt;Congratulations to Shadi!  You were the first Ph.D. student I started working with at CSU and congratulations on navigating this convoluted and sometimes challenging path successfully. Best of luck in what comes next!&lt;/p&gt;

&lt;p&gt;Here is a picture after defense, of Shadi with her committee members: Prof. Francisco Ortega (CS) and Prof. Edwin Chong (Electrical and Computer Engineering). And me (not shown, Prof. Nathaniel Blanchard, who attended online).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/fall24/shadi-defense.jpg?raw=true&quot; alt=&quot;Shadi Manafi PhD Dissertation Defense&quot; title=&quot;Shadi Manafi PhD Dissertation Defense&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(X-posted on &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/2024/06/25/congratulations-shadi-manafi.html&quot;&gt;nikhilkrishnaswamy.com&lt;/a&gt;)&lt;/p&gt;
</description>
        <pubDate>Mon, 07 Oct 2024 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//congratulations-shadi-manafi</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//congratulations-shadi-manafi</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>grads</category>
        
        
      </item>
    
      <item>
        <title>Best Paper Award at EDM 2024</title>
        <description>&lt;p&gt;Exciting news! Our paper, &lt;em&gt;Propositional Extraction from Natural Speech in Small Group Collaborative Tasks&lt;/em&gt;, was awarded Best Student Paper at the International Conference on Educational Data Mining 2024. This paper addresses the problem of extracting the semantics expressed by spoken utterances in a multiparty context. In spoken dialogue, the same semantic content can be expressed in many different ways. While it’s easy for humans to interpret the many ways of saying the same thing, it is challenging for computers for many reasons, such as filler words, disfluencies, and overlapping utterances. We adapted a cross-encoding method from coreference research in NLP to address this problem and perform significantly better than a vector-similarity approach, and achieve a strong baseline in this novel, challenging task. We also analyze the impact that transcription errors from automatic speech recognition have on performace, and find that with our cross-encoding approach, the impact of such errors can be substantially minimized.&lt;/p&gt;

&lt;p&gt;Congratulations to Videep, Abhijnan, Ibrahim, Avyakta, and Mariah!&lt;/p&gt;

&lt;p&gt;First author &lt;strong&gt;Videep Venkatesha&lt;/strong&gt; was in Atlanta this week to present the paper and accept the award. This paper was based on material funded by both the NSF iSAT institute and the DARPA FACT program, and is a collaboration with Brandeis University. The link to the proceedings can be found &lt;a href=&quot;https://educationaldatamining.org/edm2024/proceedings/2024.EDM-long-papers.14/2024.EDM-long-papers.14.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Citation: Venkatesha, V., Nath, A., Khebour, I., Chelle, A., Bradford, M., Tu, J., Pustejovsky, J., Blanchard, N., and Krishnaswamy, N. (2024). Propositional Extraction from Natural Speech in Small Group Collaborative Tasks. In &lt;em&gt;International Conference on Educational Data Mining (EDM)&lt;/em&gt;. International EDM Society.&lt;/p&gt;

&lt;p&gt;(X-posted on &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/2024/07/17/best-paper-award-edm-2024.html&quot;&gt;nikhilkrishnaswamy.com&lt;/a&gt;)&lt;/p&gt;
</description>
        <pubDate>Wed, 17 Jul 2024 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//best-paper-award-edm-2024</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//best-paper-award-edm-2024</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>awards</category>
        
        
      </item>
    
      <item>
        <title>Congratulations to Nada Alalyani!</title>
        <description>&lt;p&gt;Congratulations to Dr. &lt;strong&gt;Nada Alalyani&lt;/strong&gt; for successfully defending her Ph.D. dissertation!&lt;/p&gt;

&lt;p&gt;In Nada’s dissertation, &lt;em&gt;Embodied Multimodal Referring Expressions Generation&lt;/em&gt;, she takes the Diana system which I worked on through my postdoc and uses it to train an LLM that can drive interactive virtual avatar behavior to refer to objects in context using mixed modalities. It’s also somewhat bittersweet for me because this represents probably the last research done with the Diana system from the CwC project.&lt;/p&gt;

&lt;p&gt;Congratulations to Nada!  I’ve been continually impressed by your ability to take challenging ideas and turn them into reality. Best of luck in your future endeavors!&lt;/p&gt;

&lt;p&gt;(X-posted on &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/2024/06/25/congratulations-nada-alalyani.html&quot;&gt;nikhilkrishnaswamy.com&lt;/a&gt;)&lt;/p&gt;
</description>
        <pubDate>Tue, 25 Jun 2024 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//congratulations-nada-alalyani</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//congratulations-nada-alalyani</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>grads</category>
        
        
      </item>
    
      <item>
        <title>Here are three of us at LREC-COLING 2024</title>
        <description>&lt;p&gt;Here are three of us outside the Lingotto Conference Center in Turin, Italy, site of LREC-COLING 2024, held from May 20-25. From left to right: Shadi Manafi, Nikhil Krishnaswamy, Abhijnan Nath, and Ibrahim Khebour.&lt;/p&gt;

&lt;p&gt;We had one poster presentation and two talks, and LREC-COLING hired a (really quite good, though very loud) Queen cover band for the gala dinner.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/summer24/lrec-coling-2024.jpg?raw=true&quot; alt=&quot;LREC-COLING 2024&quot; title=&quot;LREC-COLING 2024&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 31 May 2024 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//lrec-coling-2024</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//lrec-coling-2024</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>pubs</category>
        
        
      </item>
    
      <item>
        <title>CS Department Scholarship Recipients</title>
        <description>&lt;p&gt;Three SIGNAL Lab students were awarded scholarships by the CSU CS department for meritorious achievements in AI/computer science.&lt;/p&gt;

&lt;p&gt;The recipients are: &lt;strong&gt;Abhijnan Nath&lt;/strong&gt;, winner of the Evolutionary Computation and Artificial Intelligence Graduate Fellowship; &lt;strong&gt;Sheikh Mannan&lt;/strong&gt;, winner of the Computer Science Graduate Fellowship; and &lt;strong&gt;Shadi Manafi&lt;/strong&gt;, winner of the Wim Böhm and Partners Fellowship.  Congratulations to Abhijnan, Mannan, and Shadi!&lt;/p&gt;
</description>
        <pubDate>Mon, 06 May 2024 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//cs-department-scholarship-recipients</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//cs-department-scholarship-recipients</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>awards</category>
        
        
      </item>
    
      <item>
        <title>Congratulations to Hannah VanderHoeven!</title>
        <description>&lt;p&gt;Congratulations to &lt;strong&gt;Hannah VanderHoeven&lt;/strong&gt; on successfully defending her Master’s thesis!&lt;/p&gt;

&lt;p&gt;Hannah’s thesis is entitled &lt;em&gt;Robust Gesture Detection for Multimodal Problem Solving&lt;/em&gt;, and comprises a synthesis of a few papers that appeared (or will appear) at HCII over these couple years [&lt;a href=&quot;https://www.nikhilkrishnaswamy.com/assets/docs/pdfs/HCII-2023-VanderHoeven.pdf&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/assets/docs/pdfs/HCII-2024-VanderHoeven-Point.pdf&quot;&gt;2&lt;/a&gt;, &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/assets/docs/pdfs/HCII-2024-VanderHoeven-Multimodal.pdf&quot;&gt;3&lt;/a&gt;], focusing on gesture detection techniques for use in collaborative agents for multimodal problem solving. This work originated in the iSAT project, which we recently completed a demo of to the NSF, and is being expanded for use in the FACT program.&lt;/p&gt;

&lt;p&gt;Congratulations to Hannah!  It’s been a pleasure working with you and I’m excited to see where this work goes next!&lt;/p&gt;

&lt;p&gt;I don’t have a picture of the defense, but here’s a picture from the poster session from the NSF iSAT site visit yesterday where similar work was presented. From left to right: Yifan Zhu (Brandeis University), Hannah, and SIGNAL Lab Ph.D. students Mariah Bradford and Ibrahim Khebour.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/spring24/nsf-site-visit-2024.jpg?raw=true&quot; alt=&quot;NSF Site Visit 2024&quot; title=&quot;NSF Site Visit 2024&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(X-posted on &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/2024/05/01/congratulations-hannah-vanderhoeven.html&quot;&gt;nikhilkrishnaswamy.com&lt;/a&gt;)&lt;/p&gt;
</description>
        <pubDate>Wed, 01 May 2024 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//congratulations-hannah-vanderhoeven</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//congratulations-hannah-vanderhoeven</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>grads</category>
        
        
      </item>
    
      <item>
        <title>DARPA FACT AIE Award</title>
        <description>&lt;p&gt;I am beyond excited that we have been awarded a contract on the DARPA Friction and Accountability in Conversational Transactions (FACT) AI Exploration program! Our project, &lt;em&gt;TRACE: Transparency, Reflection, and Accountability in Conversational Exchanges&lt;/em&gt;, will address a lack of “friction” (deliberation and reflective reasoning) in LLMs that prevent them from being used reliably in mission-critical workflows.&lt;/p&gt;

&lt;p&gt;Put simply, LLMs are incentivized to accept the premises in whatever is input to them, even if false or faulty, and generate responses accordingly. This is due to an inability to track and assess the validity of beliefs held by their interlocutors. Theory of Mind (ToM) provides a way of measuring and tracking such beliefs, such as in the course of a shared collaborative task. Therefore we’re going to build ToM into LLMs.&lt;/p&gt;

&lt;p&gt;This project is a collaboration with Dr. James Pustejovsky at Brandeis University. Other CSU personnel include Dr. Nate Blanchard, Dr. Sarath Sreedharan, and Dr. Bruce Draper. I’m very excited to lead this excellent team!&lt;/p&gt;

&lt;p&gt;(X-posted on &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/2024/02/23/darpa-fact-aie-award.html&quot;&gt;nikhilkrishnaswamy.com&lt;/a&gt;)&lt;/p&gt;
</description>
        <pubDate>Fri, 23 Feb 2024 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//darpa-fact-aie-award</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//darpa-fact-aie-award</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>grants</category>
        
        
      </item>
    
      <item>
        <title>LREC-COLING Hat Trick and Other Stories</title>
        <description>&lt;p&gt;We are delighted to be 3 for 3 on submissions to LREC-COLING 2024, the first Joint International Conference on Computational Linguistics and Language Resources and Evaluation! The accepted papers are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Nath, A., Jamil, H., Ahmed, S. R., Baker, G., Ghosh, R., Martin, J. H., Blanchard, N., and Krishnaswamy, N. (2024). Multimodal Cross-Document Event Coreference Resolution Using Linear Semantic Transfer and Mixed-Modality Ensembles. In &lt;em&gt;Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING).&lt;/em&gt; ACL.
(In collaboration with the University of Colorado Boulder.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Khebour, I., Lai, K., Bradford, M., Zhu, Y., Brutti, R., Tam, C., Tu, J., Ibarra, B., Blanchard, N., Krishnaswamy, N., and Pustejovsky, J. (2024). Common Ground Tracking in Multimodal Dialogue. In &lt;em&gt;Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING).&lt;/em&gt; ACL.
(In collaboration with Brandeis University.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Manafi, S. and Krishnaswamy, N. (2024). Cross-Lingual Transfer Robustness to Lower-Resource Languages on Adversarial Datasets. In &lt;em&gt;Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING).&lt;/em&gt; ACL.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The conference will be held in Turin, Italy, in late May. Congratulations to SIGNAL Lab graduate students Abhijnan, Ibrahim, Mariah, and Shadi (and all affiliated students/external collaborators)!&lt;/p&gt;

&lt;p&gt;Additionally, Sheikh Mannan and Sadaf Ghaffari have a publication each at AAAI Spring Symposia, to be held at Stanford University in March:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Ghaffari, S. and Krishnaswamy, N. (2024). Exploring Failure Cases in Multimodal Reasoning About Physical Dynamics. In &lt;em&gt;AAAI Spring Symposium: Empowering Machine Learning and Large Language Models with Domain and Commonsense Knowledge (MAKE).&lt;/em&gt; AAAI.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Mannan, S., Vimal, V. P., DiZio, P., and Krishnaswamy, N. (2024). Embodying Human-Like Modes of Balance Control Through Human-in-the-Loop Dyadic Learning. In &lt;em&gt;AAAI Spring Symposium: Symposium on Human-Like Learning (HLL).&lt;/em&gt; AAAI.
(In collaboration with Brandeis University.)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Last but not least, another hat trick (+1 bonus poster) at the International Conference on Human-Computer Interaction, to be held in Washington, DC, in June/July:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;VanderHoeven, H., Bradford, M., Jung, C., Khebour, I., Lai, K., Pustejovsky, J., Krishnaswamy, N., and Blanchard, N. (2024). Multimodal Design for Interactive Collaborative Problem Solving Support. In &lt;em&gt;International Conference on Human-Computer Interaction (HCII).&lt;/em&gt; Springer.
(In collaboration with Brandeis University.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Zhu, Y., VanderHoeven, H., Lai, K., Bradford, M., Tam, C., Khebour, I., Brutti, R., Krishnaswamy, N., and Pustejovsky, J. (2024). Modeling Theory of Mind in Multimodal HCI. In &lt;em&gt;International Conference on Human-Computer Interaction (HCII).&lt;/em&gt; Springer.
(In collaboration with Brandeis University.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;VanderHoeven, H., Blanchard, N., and Krishnaswamy, N. (2024). Point Target Detection for Multimodal Communication. In &lt;em&gt;International Conference on Human-Computer Interaction (HCII).&lt;/em&gt; Springer.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Congratulations to Hannah (x3), Mariah, and Ibrahim again!&lt;/p&gt;

&lt;p&gt;And the poster:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Seefried, E., Bradford, M., Aich, S., Siebert, C., Krishnaswamy, N., and Blanchard, N. (2024). Learning Foreign Language Vocabulary Through Task-Based Virtual Reality Immersion. In &lt;em&gt;International Conference on Human-Computer Interaction (HCII).&lt;/em&gt; Springer.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 20 Feb 2024 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//coling-hat-trick-other-stories</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//coling-hat-trick-other-stories</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>pubs</category>
        
        
      </item>
    
  </channel>
</rss>
